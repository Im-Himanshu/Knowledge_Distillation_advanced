{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1281e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neutrino.framework.torch_framework import TorchFramework\n",
    "from neutrino.job import Neutrino\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch\n",
    "framework = TorchFramework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6946bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar100_dataset(dataroot, batch_size):\n",
    "    trainset = torchvision.datasets.CIFAR100(root=dataroot,\n",
    "                                             train=True,\n",
    "                                             download=True,\n",
    "                                             transform=transforms.Compose([\n",
    "                                                 transforms.RandomCrop(32, padding=4),\n",
    "                                                 transforms.RandomHorizontalFlip(),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                      (0.2023, 0.1994, 0.2010))\n",
    "                                             ]))\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR100(root=dataroot,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                     (0.2023, 0.1994, 0.2010))\n",
    "                                            ]))\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return {\n",
    "            'train': trainloader,\n",
    "            'test': testloader\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace7d478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /mnt/nfshome/risavsingh.saingar/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01900eac9ef4335aaf014cd9645a269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_model = torchvision.models.resnet18(pretrained=True)\n",
    "reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca91afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = get_cifar100_dataset(\"./Data\", 16)\n",
    "config = {\n",
    "    \"optimization\": \"compression\",\n",
    "    \"delta\": \"1\",\n",
    "    'export': {\n",
    "        'format': ['onnx'],\n",
    "        'kwargs': {\n",
    "            'root_path': \"./Models\",\n",
    "            'precision': 'fp32', # ('fp32' or 'fp16'), only for onnx, dlrt formats\n",
    "            'resolutions': [(32, 32), (36, 36)] # list of tuples, only for onnx, dlrt formats\n",
    "        }\n",
    "    }\n",
    "}\n",
    "model = Neutrino(framework, data, model=reference_model, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adf4773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-09 08:13:01 - INFO: Starting job with ID: B5DEBCB7\n",
      "2023-01-09 08:13:01 - INFO: Args: -f, /mnt/nfshome/risavsingh.saingar/.local/share/jupyter/runtime/kernel-927196e7-204f-4f91-9926-b6a75a11c718.json\n",
      "2023-01-09 08:13:01 - INFO: \n",
      "+------------------------------------------------------------------------------------+\n",
      "| Neutrino 5.3.3                                                 09/01/2023 08:13:01 |\n",
      "+------------------------------------------------------------------------------------+\n",
      "| License                                                    HVHEP-GCXFZ-KGUTB-TVGPI |\n",
      "| Status                                                                       Valid |\n",
      "| Expires                                                        2024-01-09 07:14:07 |\n",
      "| Features                                                                           |\n",
      "+------------------------------------------------------------------------------------+\n",
      "\n",
      "2023-01-09 08:13:01 - INFO: Neutrino Caching disabled\n",
      "2023-01-09 08:13:01 - INFO: Verifying deeplite-profiler package signature\n",
      "2023-01-09 08:13:03 - INFO: Entering compression optimization mode\n",
      "2023-01-09 08:13:03 - INFO: Backend: TorchBackend\n",
      "2023-01-09 08:13:03 - INFO: Parsed task type <Task.CLS: 'classification'>\n",
      "2023-01-09 08:13:06 - INFO: Trying forward passes on training data...\n",
      "2023-01-09 08:13:08 - INFO: Training Loop interface validated\n",
      "2023-01-09 08:13:08 - INFO: ...Success\n",
      "2023-01-09 08:13:08 - INFO: Test dataset size: 10000 instances\n",
      "2023-01-09 08:13:08 - INFO: Train dataset size: 50000 instances\n",
      "2023-01-09 08:13:08 - INFO: Exporting to ONNX format\n",
      "PyTorch2ONNX converted model ModelFormat.PYTORCH to ModelFormat.ONNX\n",
      "2023-01-09 08:13:09 - INFO: Exporting to pkl format\n",
      "2023-01-09 08:13:10 - INFO: Model has been exported to pkl format: /mnt/nfshome/risavsingh.saingar/Current/Edgification/Models/ref_model.pkl\n",
      "2023-01-09 08:13:10 - INFO: Computing network status...\n",
      "2023-01-09 08:13:19 - INFO: \n",
      "+---------------------------------------------------------------+\n",
      "|                       Deeplite Profiler                       |\n",
      "+-----------------------------------------+---------------------+\n",
      "|            Param Name (Reference Model) |                Value|\n",
      "|                   Backend: TorchBackend |                     |\n",
      "+-----------------------------------------+---------------------+\n",
      "|                   Evaluation Metric (%) |               0.1400|\n",
      "|                         Model Size (MB) |              44.5919|\n",
      "|     Computational Complexity (GigaMACs) |               0.0377|\n",
      "|             Total Parameters (Millions) |              11.6895|\n",
      "|                   Memory Footprint (MB) |              45.5586|\n",
      "+-----------------------------------------+---------------------+\n",
      "\n",
      "2023-01-09 08:13:19 - ERROR: Ooops something went wrong, job id B5DEBCB7\n",
      "2023-01-09 08:13:19 - ERROR: TypeError: unsupported operand type(s) for -: 'float' and 'str'\n",
      " \n",
      "2023-01-09 08:13:19 - INFO: Job with ID B5DEBCB7 finished\n",
      "2023-01-09 08:13:19 - INFO: Total execution time: 0:00:18 (d, hh:mm:ss)\n",
      "2023-01-09 08:13:19 - INFO: Log has been exported to: /mnt/nfshome/risavsingh.saingar/.neutrino/logs/B5DEBCB7-2023-01-09.elog\n"
     ]
    }
   ],
   "source": [
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d3ca226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (Conv_0): Conv2d(3, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_0): ReLU()\n",
       "  (Conv_1): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_1): ReLU()\n",
       "  (Conv_2): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_0): OnnxBinaryMathOperation()\n",
       "  (Relu_2): ReLU()\n",
       "  (Conv_3): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_3): ReLU()\n",
       "  (Conv_4): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_1): OnnxBinaryMathOperation()\n",
       "  (Relu_4): ReLU()\n",
       "  (Conv_5): Conv2d(64, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_5): ReLU()\n",
       "  (Conv_6): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_7): Conv2d(64, 128, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_2): OnnxBinaryMathOperation()\n",
       "  (Relu_6): ReLU()\n",
       "  (Conv_8): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_7): ReLU()\n",
       "  (Conv_9): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_3): OnnxBinaryMathOperation()\n",
       "  (Relu_8): ReLU()\n",
       "  (Conv_10): Conv2d(128, 256, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_9): ReLU()\n",
       "  (Conv_11): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_12): Conv2d(128, 256, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_4): OnnxBinaryMathOperation()\n",
       "  (Relu_10): ReLU()\n",
       "  (Conv_13): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_11): ReLU()\n",
       "  (Conv_14): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_5): OnnxBinaryMathOperation()\n",
       "  (Relu_12): ReLU()\n",
       "  (Conv_15): Conv2d(256, 512, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_13): ReLU()\n",
       "  (Conv_16): Conv2d(512, 512, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_17): Conv2d(256, 52, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], bias=False)\n",
       "  (Conv_18): Conv2d(52, 104, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1], bias=False)\n",
       "  (Conv_19): Conv2d(104, 512, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_6): OnnxBinaryMathOperation()\n",
       "  (Relu_14): ReLU()\n",
       "  (Conv_20): Conv2d(512, 182, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], bias=False)\n",
       "  (Conv_21): Conv2d(182, 172, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], bias=False)\n",
       "  (Conv_22): Conv2d(172, 512, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
       "  (Relu_15): ReLU()\n",
       "  (Conv_23): Conv2d(512, 186, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], bias=False)\n",
       "  (Conv_24): Conv2d(186, 220, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], bias=False)\n",
       "  (Conv_25): Conv2d(220, 512, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_7): OnnxBinaryMathOperation()\n",
       "  (Relu_16): ReLU()\n",
       "  (Constant_0): OnnxConstant()\n",
       "  (Pad_0): OnnxPadDynamic()\n",
       "  (AveragePool_0): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=[0, 0])\n",
       "  (Shape_0): OnnxShape()\n",
       "  (Constant_1): OnnxConstant()\n",
       "  (Gather_0): OnnxGather()\n",
       "  (Unsqueeze_0): OnnxUnsqueezeStaticAxes()\n",
       "  (initializers): Module()\n",
       "  (Concat_0): OnnxConcat()\n",
       "  (Reshape_0): OnnxReshape()\n",
       "  (Gemm_0): OnnxGemm()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2torch import convert\n",
    "m = onnx.load(\"./second_best_modelfp32_dynamic_shape.onnx\")\n",
    "convert(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d38e01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (Conv_0): Conv2d(3, 64, kernel_size=[7, 7], stride=[2, 2], padding=[3, 3], dilation=[1, 1])\n",
       "  (Relu_0): ReLU()\n",
       "  (MaxPool_0): MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=1, ceil_mode=False)\n",
       "  (Conv_1): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_1): ReLU()\n",
       "  (Conv_2): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_0): OnnxBinaryMathOperation()\n",
       "  (Relu_2): ReLU()\n",
       "  (Conv_3): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_3): ReLU()\n",
       "  (Conv_4): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_1): OnnxBinaryMathOperation()\n",
       "  (Relu_4): ReLU()\n",
       "  (Conv_5): Conv2d(64, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_5): ReLU()\n",
       "  (Conv_6): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_7): Conv2d(64, 128, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_2): OnnxBinaryMathOperation()\n",
       "  (Relu_6): ReLU()\n",
       "  (Conv_8): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_7): ReLU()\n",
       "  (Conv_9): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_3): OnnxBinaryMathOperation()\n",
       "  (Relu_8): ReLU()\n",
       "  (Conv_10): Conv2d(128, 256, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_9): ReLU()\n",
       "  (Conv_11): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_12): Conv2d(128, 256, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_4): OnnxBinaryMathOperation()\n",
       "  (Relu_10): ReLU()\n",
       "  (Conv_13): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_11): ReLU()\n",
       "  (Conv_14): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_5): OnnxBinaryMathOperation()\n",
       "  (Relu_12): ReLU()\n",
       "  (Conv_15): Conv2d(256, 512, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_13): ReLU()\n",
       "  (Conv_16): Conv2d(512, 512, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_17): Conv2d(256, 512, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_6): OnnxBinaryMathOperation()\n",
       "  (Relu_14): ReLU()\n",
       "  (Conv_18): Conv2d(512, 512, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_15): ReLU()\n",
       "  (Conv_19): Conv2d(512, 512, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_7): OnnxBinaryMathOperation()\n",
       "  (Relu_16): ReLU()\n",
       "  (GlobalAveragePool_0): OnnxGlobalAveragePoolWithKnownInputShape()\n",
       "  (Flatten_0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (initializers): Module()\n",
       "  (Gemm_0): OnnxGemm()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2torch import convert\n",
    "m = onnx.load(\"./Models/ref_modelfp32_dynamic_shape.onnx\")\n",
    "convert(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c674a415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ModelProto in module onnx.onnx_ml_pb2 object:\n",
      "\n",
      "class ModelProto(google.protobuf.pyext._message.CMessage, google.protobuf.message.Message)\n",
      " |  A ProtocolMessage\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModelProto\n",
      " |      google.protobuf.pyext._message.CMessage\n",
      " |      google.protobuf.message.Message\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  doc_string\n",
      " |      Field onnx.ModelProto.doc_string\n",
      " |  \n",
      " |  domain\n",
      " |      Field onnx.ModelProto.domain\n",
      " |  \n",
      " |  functions\n",
      " |      Field onnx.ModelProto.functions\n",
      " |  \n",
      " |  graph\n",
      " |      Field onnx.ModelProto.graph\n",
      " |  \n",
      " |  ir_version\n",
      " |      Field onnx.ModelProto.ir_version\n",
      " |  \n",
      " |  metadata_props\n",
      " |      Field onnx.ModelProto.metadata_props\n",
      " |  \n",
      " |  model_version\n",
      " |      Field onnx.ModelProto.model_version\n",
      " |  \n",
      " |  opset_import\n",
      " |      Field onnx.ModelProto.opset_import\n",
      " |  \n",
      " |  producer_name\n",
      " |      Field onnx.ModelProto.producer_name\n",
      " |  \n",
      " |  producer_version\n",
      " |      Field onnx.ModelProto.producer_version\n",
      " |  \n",
      " |  training_info\n",
      " |      Field onnx.ModelProto.training_info\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DESCRIPTOR = <google.protobuf.pyext._message.MessageDescriptor object>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  ByteSize(...)\n",
      " |      Returns the size of the message in bytes.\n",
      " |  \n",
      " |  Clear(...)\n",
      " |      Clears the message.\n",
      " |  \n",
      " |  ClearExtension(...)\n",
      " |      Clears a message field.\n",
      " |  \n",
      " |  ClearField(...)\n",
      " |      Clears a message field.\n",
      " |  \n",
      " |  CopyFrom(...)\n",
      " |      Copies a protocol message into the current message.\n",
      " |  \n",
      " |  DiscardUnknownFields(...)\n",
      " |      Discards the unknown fields.\n",
      " |  \n",
      " |  FindInitializationErrors(...)\n",
      " |      Finds unset required fields.\n",
      " |  \n",
      " |  HasExtension(...)\n",
      " |      Checks if a message field is set.\n",
      " |  \n",
      " |  HasField(...)\n",
      " |      Checks if a message field is set.\n",
      " |  \n",
      " |  IsInitialized(...)\n",
      " |      Checks if all required fields of a protocol message are set.\n",
      " |  \n",
      " |  ListFields(...)\n",
      " |      Lists all set fields of a message.\n",
      " |  \n",
      " |  MergeFrom(...)\n",
      " |      Merges a protocol message into the current message.\n",
      " |  \n",
      " |  MergeFromString(...)\n",
      " |      Merges a serialized message into the current message.\n",
      " |  \n",
      " |  ParseFromString(...)\n",
      " |      Parses a serialized message into the current message.\n",
      " |  \n",
      " |  SerializePartialToString(...)\n",
      " |      Serializes the message to a string, even if it isn't initialized.\n",
      " |  \n",
      " |  SerializeToString(...)\n",
      " |      Serializes the message to a string, only for initialized messages.\n",
      " |  \n",
      " |  SetInParent(...)\n",
      " |      Sets the has bit of the given field in its parent message.\n",
      " |  \n",
      " |  UnknownFields(...)\n",
      " |      Parse unknown field set\n",
      " |  \n",
      " |  WhichOneof(...)\n",
      " |      Returns the name of the field set inside a oneof, or None if no field is set.\n",
      " |  \n",
      " |  __deepcopy__(...)\n",
      " |      Makes a deep copy of the class.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |      Outputs a unicode representation of the message.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  FromString(...) from google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\n",
      " |      Creates new method instance from given serialized data.\n",
      " |  \n",
      " |  RegisterExtension(...) from google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\n",
      " |      Registers an extension with the current message.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from google.protobuf.pyext._message.MessageMeta\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  Extensions\n",
      " |      Extension dict\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.protobuf.message.Message:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Support the pickle protocol.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |      Support the pickle protocol.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e0c5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f472cf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (Conv_0): Conv2d(3, 64, kernel_size=[7, 7], stride=[2, 2], padding=[3, 3], dilation=[1, 1])\n",
       "  (Relu_0): ReLU()\n",
       "  (MaxPool_0): MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=1, ceil_mode=False)\n",
       "  (Conv_1): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_1): ReLU()\n",
       "  (Conv_2): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_0): OnnxBinaryMathOperation()\n",
       "  (Relu_2): ReLU()\n",
       "  (Conv_3): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_3): ReLU()\n",
       "  (Conv_4): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_1): OnnxBinaryMathOperation()\n",
       "  (Relu_4): ReLU()\n",
       "  (Conv_5): Conv2d(64, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_5): ReLU()\n",
       "  (Conv_6): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_7): Conv2d(64, 128, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_2): OnnxBinaryMathOperation()\n",
       "  (Relu_6): ReLU()\n",
       "  (Conv_8): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_7): ReLU()\n",
       "  (Conv_9): Conv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_3): OnnxBinaryMathOperation()\n",
       "  (Relu_8): ReLU()\n",
       "  (Conv_10): Conv2d(128, 256, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_9): ReLU()\n",
       "  (Conv_11): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_12): Conv2d(128, 256, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_4): OnnxBinaryMathOperation()\n",
       "  (Relu_10): ReLU()\n",
       "  (Conv_13): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_11): ReLU()\n",
       "  (Conv_14): Conv2d(256, 256, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_5): OnnxBinaryMathOperation()\n",
       "  (Relu_12): ReLU()\n",
       "  (Conv_15): Conv2d(256, 512, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_13): ReLU()\n",
       "  (Conv_16): Conv2d(512, 512, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Conv_17): Conv2d(256, 512, kernel_size=[1, 1], stride=[2, 2], padding=[0, 0], dilation=[1, 1])\n",
       "  (Add_6): OnnxBinaryMathOperation()\n",
       "  (Relu_14): ReLU()\n",
       "  (Conv_18): Conv2d(512, 512, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Relu_15): ReLU()\n",
       "  (Conv_19): Conv2d(512, 512, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1])\n",
       "  (Add_7): OnnxBinaryMathOperation()\n",
       "  (Relu_16): ReLU()\n",
       "  (GlobalAveragePool_0): OnnxGlobalAveragePoolWithKnownInputShape()\n",
       "  (Flatten_0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (initializers): Module()\n",
       "  (Gemm_0): OnnxGemm()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24263ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loaded 122/122 modules\n",
      "2023-01-09 08:33:14 - INFO: Starting job with ID: B5DEBCB7\n",
      "2023-01-09 08:33:14 - INFO: Args: -f, /mnt/nfshome/risavsingh.saingar/.local/share/jupyter/runtime/kernel-927196e7-204f-4f91-9926-b6a75a11c718.json\n",
      "2023-01-09 08:33:14 - INFO: \n",
      "+------------------------------------------------------------------------------------+\n",
      "| Neutrino 5.3.3                                                 09/01/2023 08:33:14 |\n",
      "+------------------------------------------------------------------------------------+\n",
      "| License                                                    HVHEP-GCXFZ-KGUTB-TVGPI |\n",
      "| Status                                                                       Valid |\n",
      "| Expires                                                        2024-01-09 07:14:07 |\n",
      "| Features                                                                           |\n",
      "+------------------------------------------------------------------------------------+\n",
      "\n",
      "2023-01-09 08:33:14 - INFO: Neutrino Caching disabled\n",
      "2023-01-09 08:33:14 - INFO: Verifying deeplite-profiler package signature\n",
      "2023-01-09 08:33:17 - INFO: --***-- THIS IS A DRYRUN --***--\n",
      "2023-01-09 08:33:17 - INFO: Entering compression optimization mode\n",
      "2023-01-09 08:33:17 - INFO: Backend: TorchBackend\n",
      "2023-01-09 08:33:17 - INFO: Parsed task type <Task.CLS: 'classification'>\n",
      "2023-01-09 08:33:18 - INFO: Trying forward passes on training data...\n",
      "2023-01-09 08:33:19 - INFO: Training Loop interface validated\n",
      "2023-01-09 08:33:19 - INFO: ...Success\n",
      "2023-01-09 08:33:19 - INFO: Test dataset size: 10000 instances\n",
      "2023-01-09 08:33:19 - INFO: Train dataset size: 50000 instances\n",
      "2023-01-09 08:33:19 - INFO: Exporting to ONNX format\n",
      "PyTorch2ONNX converted model ModelFormat.PYTORCH to ModelFormat.ONNX\n",
      "2023-01-09 08:33:21 - INFO: Exporting to pkl format\n",
      "2023-01-09 08:33:21 - INFO: Model has been exported to pkl format: /mnt/nfshome/risavsingh.saingar/Current/Edgification/ref_model.pkl\n",
      "2023-01-09 08:33:21 - INFO: Computing network status...\n",
      "2023-01-09 08:33:24 - INFO: \n",
      "+---------------------------------------------------------------+\n",
      "|                       Deeplite Profiler                       |\n",
      "+-----------------------------------------+---------------------+\n",
      "|            Param Name (Reference Model) |                Value|\n",
      "|                   Backend: TorchBackend |                     |\n",
      "+-----------------------------------------+---------------------+\n",
      "|                   Evaluation Metric (%) |              76.9200|\n",
      "|                         Model Size (MB) |              42.8014|\n",
      "|     Computational Complexity (GigaMACs) |               0.5567|\n",
      "|             Total Parameters (Millions) |              11.2201|\n",
      "|                   Memory Footprint (MB) |              48.8022|\n",
      "+-----------------------------------------+---------------------+\n",
      "\n",
      "2023-01-09 08:33:24 - INFO: This is a compression styled composition\n",
      "2023-01-09 08:33:25 - INFO: Analyzing design space...\n",
      "2023-01-09 08:33:25 - INFO: \n",
      "+------------------------------------------------------------------------------------+\n",
      "|                                  Target |                           75.92 accuracy |\n",
      "+------------------------------------------------------------------------------------+\n",
      "|                           At most steps |                                        8 |\n",
      "+------------------------------------------------------------------------------------+\n",
      "|              Estimated exploration time |                    0:08:26 (d, hh:mm:ss) |\n",
      "+------------------------------------------------------------------------------------+\n",
      "2023-01-09 08:33:25 - INFO: Phase 1\n",
      "2023-01-09 08:33:25 - INFO: Step 1\n",
      "2023-01-09 08:34:57 - INFO: Step 2\n",
      "2023-01-09 08:36:28 - INFO: Step 3\n",
      "2023-01-09 08:37:56 - INFO: Step 4\n",
      "2023-01-09 08:39:25 - INFO: Step 5\n",
      "2023-01-09 08:40:46 - INFO: Step 6\n",
      "2023-01-09 08:42:05 - INFO: Step 7\n",
      "2023-01-09 08:43:21 - INFO: Step 8\n",
      "2023-01-09 08:44:24 - INFO: Step 9\n",
      "2023-01-09 08:45:08 - INFO: Step 10\n",
      "2023-01-09 08:46:19 - INFO: Phase 2\n",
      "2023-01-09 08:46:19 - INFO: Step 1\n",
      "2023-01-09 08:46:27 - INFO: Step 2\n",
      "2023-01-09 08:46:34 - INFO: Step 3\n",
      "2023-01-09 08:46:42 - INFO: Step 4\n",
      "2023-01-09 08:46:49 - INFO: Step 5\n",
      "2023-01-09 08:47:23 - INFO: Comparing networks status...\n",
      "2023-01-09 08:47:28 - INFO: \n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                    Deeplite Profiler                                                     |\n",
      "+-----------------------------------------+--------------------------+--------------------------+--------------------------+\n",
      "|                              Param Name |               Enhancement|   Value (Optimized Model)|   Value (Reference Model)|\n",
      "|                                         |                          |     Backend: TorchBackend|     Backend: TorchBackend|\n",
      "+-----------------------------------------+--------------------------+--------------------------+--------------------------+\n",
      "|                   Evaluation Metric (%) |                     -2.37|                   74.5500|                   76.9200|\n",
      "|                         Model Size (MB) |                     1.49x|                   28.7654|                   42.8014|\n",
      "|     Computational Complexity (GigaMACs) |                     1.12x|                    0.4978|                    0.5567|\n",
      "|             Total Parameters (Millions) |                     1.49x|                    7.5407|                   11.2201|\n",
      "|                   Memory Footprint (MB) |                     1.40x|                   34.8125|                   48.8022|\n",
      "+-----------------------------------------+--------------------------+--------------------------+--------------------------+\n",
      "\n",
      "2023-01-09 08:47:28 - INFO: Exporting the second best model, in order to get the best model please upgrade at https://www.deeplite.ai/production-license\n",
      "2023-01-09 08:47:28 - INFO: Exporting to ONNX format\n",
      "PyTorch2ONNX converted model ModelFormat.PYTORCH to ModelFormat.ONNX\n",
      "2023-01-09 08:47:30 - INFO: Exporting to pkl format\n",
      "2023-01-09 08:47:30 - INFO: Model has been exported to pkl format: /mnt/nfshome/risavsingh.saingar/Current/Edgification/second_best_model.pkl\n",
      "2023-01-09 08:47:30 - INFO: Unfortunately Neutrino was not able to optimize the reference model\n",
      "2023-01-09 08:47:30 - INFO: Job with ID B5DEBCB7 finished\n",
      "2023-01-09 08:47:30 - INFO: Total execution time: 0:14:16 (d, hh:mm:ss)\n",
      "2023-01-09 08:47:30 - INFO: Log has been exported to: /mnt/nfshome/risavsingh.saingar/.neutrino/logs/B5DEBCB7-2023-01-09.elog\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from deeplite_torch_zoo import get_data_splits_by_name, get_model_by_name\n",
    "from neutrino.framework.torch_framework import TorchFramework\n",
    "from neutrino.job import Neutrino\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# # model/dataset args\n",
    "# parser.add_argument('--dataset', metavar='DATASET', default='cifar100', help='dataset to use')\n",
    "# parser.add_argument('-r', '--data_root', metavar='PATH', default='', help='dataset data root path')\n",
    "# parser.add_argument('-b', '--batch_size', type=int, metavar='N', default=128, help='mini-batch size')\n",
    "# parser.add_argument('-j', '--workers', type=int, metavar='N', default=4, help='number of data loading workers')\n",
    "# parser.add_argument('-a', '--arch', metavar='ARCH', default='resnet18', help='model architecture')\n",
    "\n",
    "# # neutrino args\n",
    "# parser.add_argument('-d', '--delta', type=float, metavar='DELTA', default=1, help='accuracy drop tolerance')\n",
    "# parser.add_argument('-l', '--level', type=int, default=1, help='level', choices=(1, 2))\n",
    "# parser.add_argument('-o', '--optimization', type=str, default='compression', choices=('compression', 'latency'))\n",
    "# parser.add_argument('--deepsearch', action='store_true', help=\"to consume the delta as much as possible\")\n",
    "# parser.add_argument('--fp16', action='store_true', help=\"export to fp16 as well if it is possible\")\n",
    "# parser.add_argument('--dryrun', action='store_true', help=\"force all loops to early break\")\n",
    "# parser.add_argument('--horovod', action='store_true', help=\"activate horovod\")\n",
    "# parser.add_argument('--device', type=str, metavar='DEVICE', default='GPU', help='Device to use, CPU or GPU')\n",
    "# parser.add_argument('--lr', default=0.1, type=float,\n",
    "#                     help='learning rate for training model. This LR is internally scaled by num gpus during distributed training')\n",
    "# parser.add_argument('--ft_lr', default=0.01, type=float, help='learning rate during fine-tuning iterations')\n",
    "# parser.add_argument('--ft_epochs', default=2, type=int, help='number of fine-tuning epochs')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "device_map = {'CPU': 'cpu', 'GPU': 'cuda'}\n",
    "\n",
    "data_splits = get_data_splits_by_name(dataset_name='cifar100',\n",
    "                                      model_name='resnet18',\n",
    "                                      data_root='./Data',\n",
    "                                      batch_size=16,\n",
    "                                      num_workers=4,\n",
    "                                      device=device_map['GPU'])\n",
    "\n",
    "reference_model = get_model_by_name(model_name='resnet18',\n",
    "                                    dataset_name='cifar100',\n",
    "                                    pretrained=True,\n",
    "                                    progress=True,\n",
    "                                    device=device_map['GPU'])\n",
    "\n",
    "config = {\n",
    "    'deepsearch': True,\n",
    "    'level': 1,\n",
    "    'delta': 1,\n",
    "    'device': 'GPU',\n",
    "    'optimization': 'compression',\n",
    "    'use_horovod': False,\n",
    "    'export': {\n",
    "        'format': ['onnx'],\n",
    "        'root_path': \"./Models\",\n",
    "        'kwargs': {'precision': 'fp16' if False else 'fp32'}\n",
    "    },\n",
    "    'full_trainer': {'optimizer': {'lr': 0.01}},\n",
    "    'fine_tuner': {\n",
    "        'loop_params': {\n",
    "            'epochs': 2,\n",
    "            'optimizer': {'lr': 0.01}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "optimized_model = Neutrino(framework=TorchFramework(),\n",
    "                           data=data_splits,\n",
    "                           model=reference_model,\n",
    "                           config=config).run(dryrun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff197b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b9901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = MNIST(\"./Data/\", download=True, train=False, transform=transforms.ToTensor())\n",
    "data_test = DataLoader(dataset_test, batch_size=8, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d51d298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–‹                                                                                               | 9/1250 [00:08<18:51,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.061826705932617 0.05769681930541992 8.207448244094849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices()[1], True)\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices()[2], True)\n",
    "w = h = 464\n",
    "class ModelTest1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelTest1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 512, kernel_size=1, padding=\"same\")\n",
    "        self.conv2_1 = nn.Conv2d(512, 20, kernel_size=1, padding=\"same\")\n",
    "        self.conv2_2 = nn.Conv2d(20, 20, kernel_size=2, padding=\"same\")\n",
    "        self.conv2_ = nn.Conv2d(20, 512, kernel_size=1, padding=\"same\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "#         print(out.shape)\n",
    "        out = self.conv2_1(out)\n",
    "#         print(out.shape)\n",
    "        out = self.conv2_2(out)\n",
    "#         print(out.shape)\n",
    "        out = self.conv2_(out)\n",
    "#         print(out.shape)\n",
    "        return out\n",
    "    \n",
    "class ModelTest2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelTest2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 512, kernel_size=1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(512, 512, kernel_size=2, padding=\"same\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "#         print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "#         print(out.shape)\n",
    "        return out\n",
    "    \n",
    "m1 = ModelTest1().to(device)\n",
    "m2 = ModelTest2().to(device)\n",
    "\n",
    "from time import time\n",
    "count = 0\n",
    "t1 = 0\n",
    "t2 = 0\n",
    "start_ = time()\n",
    "resize = tf.keras.layers.Resizing(w, h)\n",
    "for d in tqdm(data_test):\n",
    "    count += 1\n",
    "    start = time()\n",
    "    img, label = d\n",
    "    img_ = img.to(device)\n",
    "    img_ = img.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    img_ = torch.from_numpy(resize(img_).numpy()).permute(0, 3, 1, 2).to(device)\n",
    "    end = time()\n",
    "    t2 += end - start\n",
    "#     start = time()\n",
    "#     _ = m2(img_)\n",
    "#     end = time()\n",
    "#     t1 += end - start\n",
    "    start = time()\n",
    "    _ = m1(img_)\n",
    "    end = time()\n",
    "    t1 += end - start\n",
    "    if count == 10:\n",
    "        break\n",
    "end = time()\n",
    "print(t1, t2, end - start_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.17606830596923828 1.9920427799224854 2.6187398433685303\n",
    "0.017694950103759766 2.270150899887085 2.825688362121582\n",
    "\n",
    "0.04861807823181152 37.70055794715881 38.31855249404907\n",
    "0.20110821723937988 17.15498375892639 17.911318063735962\n",
    "0.03659319877624512 16.53700351715088 17.155285596847534\n",
    "0.49880242347717285 31.288609504699707 32.37795567512512\n",
    "\n",
    "0.05565214157104492 30.639862775802612 31.320554733276367\n",
    "0.0524594783782959 9.237820863723755 9.866660594940186\n",
    "\n",
    "#CPU\n",
    "14.253225803375244 0.04387092590332031 15.510392189025879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b3d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92da0afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/nfshome/risavsingh.saingar/Current/Edgification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1557057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
